# -*- coding: utf-8 -*-
import os
import sys
import asyncio
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import List

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° pythonpath
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from llm_structured_extract import async_extract_to_model
from llm_structured_extract.core.extract import _get_adapter
from llm_structured_extract.utils.logger import get_logger

logger = get_logger(__name__)

# é»˜è®¤è¦æå–çš„ 8 ä¸ªæ ¸å¿ƒæ¨¡å‹
CORE_SCHEMAS = [
    "company_basic_view",
    "company_core_business_view",
    "company_core_strategy_and_management_view",
    "company_financial_analysis_view",
    "company_founder_and_team_view",
    "company_funding_plan_view",
    "company_industry_view",
    "company_performance_and_valuation_view"
]

async def process_schema(text: str, schema: str, output_dir: Path, cache_id: str = None):
    """å¤„ç†å•ä¸ª Schema çš„æå–ä»»åŠ¡"""
    logger.info(f"ğŸš€ å¼€å§‹æå– Schema: {schema}")
    
    # å‡†å¤‡è¾“å‡ºæ–‡ä»¶è·¯å¾„
    raw_md_path = output_dir / "raw_markdown" / f"{schema}.md"
    json_path = output_dir / "parsed_json" / f"{schema}.json"
    
    try:
        # æ‰§è¡Œå¼‚æ­¥æå–
        result_obj = await async_extract_to_model(
            text, 
            schema, 
            save_raw_to=str(raw_md_path),
            context_cache_id=cache_id
        )
        
        # ä¿å­˜è§£æåçš„ JSON
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(
                result_obj.model_dump(mode='json'),
                f,
                ensure_ascii=False,
                indent=2
            )
        
        logger.info(f"âœ… Schema {schema} æå–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"âŒ Schema {schema} æå–å¤±è´¥: {str(e)}")
        return False

async def main():
    parser = argparse.ArgumentParser(description="Batch extraction for multiple schemas from a single input file.")
    parser.add_argument("input", help="Path to the input Markdown file.")
    parser.add_argument("--output-root", default="outputs", help="Root directory for outputs.")
    parser.add_argument("--use-cache", action="store_true", help="Enable context caching to save tokens.")
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"Error: Input file '{args.input}' not found.")
        sys.exit(1)
        
    with open(input_path, 'r', encoding='utf-8') as f:
        text = f.read()
        
    # åˆ›å»ºæœ¬æ¬¡æå–çš„ä¸“ç”¨æ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder_name = f"extract_{input_path.stem}_{timestamp}"
    output_dir = Path(args.output_root) / folder_name
    
    (output_dir / "raw_markdown").mkdir(parents=True, exist_ok=True)
    (output_dir / "parsed_json").mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*80}")
    print(f"ğŸ“‚ ä»»åŠ¡å¯åŠ¨: {input_path.name}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"{'='*80}\n")

    # 1. å¦‚æœå¯ç”¨äº†ç¼“å­˜ï¼Œå…ˆåˆ›å»º Context Cache
    cache_id = None
    if args.use_cache:
        try:
            adapter = _get_adapter()
            print("â³ æ­£åœ¨åˆ›å»º Context Cache (å¯èƒ½éœ€è¦å‡ åç§’)...")
            cache_id = adapter.create_context_cache(text)
            if cache_id:
                print(f"âœ¨ Cache åˆ›å»ºæˆåŠŸ: {cache_id}")
            else:
                print("âš ï¸ è¯¥é€‚é…å™¨ä¸æ”¯æŒ Context Cacheï¼Œå°†æŒ‰æ™®é€šæ¨¡å¼ç»§ç»­ã€‚")
        except Exception as e:
            print(f"âš ï¸ Cache åˆ›å»ºå¤±è´¥: {e}")

    # 2. å¹¶è¡Œæ‰§è¡Œ 8 ä¸ªæ¨¡å‹çš„æå–
    tasks = [
        process_schema(text, schema, output_dir, cache_id) 
        for schema in CORE_SCHEMAS
    ]
    
    results = await asyncio.gather(*tasks)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if r)
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ä»»åŠ¡æ€»ç»“:")
    print(f"âœ… æˆåŠŸ: {success_count} / {len(CORE_SCHEMAS)}")
    print(f"ğŸ“‚ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    asyncio.run(main())
